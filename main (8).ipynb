{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0101e92",
   "metadata": {},
   "source": [
    "# Notebook de Asistente de Trámites Municipales (RAG con LangChain y Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5da4a",
   "metadata": {},
   "source": [
    "Este notebook contiene el código para crear un sistema RAG (Retrieval-Augmented Generation) que indexa documentos PDF de trámites de Formosa y responde preguntas utilizando Gemini (Google), Ollama para embeddings y ChromaDB para la base de datos vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05430e54",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Ejecuta esta celda una sola vez para asegurar que tienes todas las librerías necesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16983aad",
   "metadata": {},
   "source": [
    "pip install langchain langchain-google-genai pypdf chromadb langchain-ollama langchain-text-splitters python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8536b4",
   "metadata": {},
   "source": [
    "## 2. Configuración y Utilidades Base\n",
    "\n",
    "Esta celda configura la clave API de Google, define la ubicación de los documentos (./tramites) y establece el modelo de embedding de Ollama.\n",
    "\n",
    "Importante: Asegúrate de que tu archivo .env contenga la línea API_KEY=\"TU_CLAVE_DE_GEMINI\" y que el servidor Ollama esté activo y tenga el modelo nomic-embed-text:latest descargado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d376c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Dependencias de LangChain\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44182f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "carpeta = \"./tramites\" # Carpeta donde deben estar tus PDFs\n",
    "\n",
    "if not os.path.exists(carpeta):\n",
    "    os.makedirs(carpeta)\n",
    "    print(f\" Carpeta '{carpeta}' creada. Por favor, coloque sus PDFs dentro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a8fee",
   "metadata": {},
   "source": [
    "### Funciones de Embedding y Vector Store\n",
    "\n",
    "Usaremos Ollama como nuestro modelo de embedding local y Chroma como vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e37ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama Embeddings configurado.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embedding = OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text:latest\"\n",
    "    )\n",
    "    print(\" Ollama Embeddings configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\" ERROR: Falló la configuración de Ollama. ¿Está el servidor activo? {e}\")\n",
    "\n",
    "\n",
    "def get_vector_store(name_collection: str): \n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        collection_name=name_collection,\n",
    "        embedding_function=embedding,\n",
    "        persist_directory=\"./vectorstore\"\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f2f7a",
   "metadata": {},
   "source": [
    "## 3. Funciones de Carga y Segmentación (Chunking)\n",
    "\n",
    "Definimos las funciones que extraen el texto de los PDFs y lo dividen en fragmentos lógicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef09931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(path: str):\n",
    "    \n",
    "    try:\n",
    "        loader = PyMuPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        # Mejor práctica: unir el contenido de las páginas directamente\n",
    "        text = \"\\n\".join([p.page_content for p in pages])\n",
    "        print(f\" PDF cargado correctamente ({len(pages)} páginas, {len(text)} caracteres)\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # Esto captura errores si el PDF está corrupto o protegido\n",
    "        print(f\" Error al cargar PDF {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text: str): \n",
    "    \n",
    "    # RecursiveCharacterTextSplitter respeta la estructura de documentos (saltos de línea)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"] # Separadores jerárquicos\n",
    "    )\n",
    "    docs = splitter.create_documents([text])\n",
    "    print(f\" {len(docs)} fragmentos creados.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def retrieval(input_user: str): \n",
    "    \n",
    "    vector_store = get_vector_store(\"tramites_formosa\")\n",
    "    # Aumentar k a 10 para obtener más contexto\n",
    "    docs = vector_store.similarity_search(input_user, k=10) \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc365d5",
   "metadata": {},
   "source": [
    "## 4. Fase de Indexación: Leer, Dividir y Guardar\n",
    "\n",
    "Este es el bloque procesa todos los archivos en la carpeta ./tramites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e392d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO FASE DE INDEXACIÓN ---\n",
      "\n",
      " Leyendo: ASPECTOS LEGALES DE LA DOCUMENTACIÓN A PRESENTAR.pdf\n",
      " PDF cargado correctamente (1 páginas, 3266 caracteres)\n",
      " 2 fragmentos creados.\n",
      " Fragmentos de ASPECTOS LEGALES DE LA DOCUMENTACIÓN A PRESENTAR.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: DOCUMENTACIÓN A PRESENTAR PARA OBRA NUEVA.pdf\n",
      " PDF cargado correctamente (3 páginas, 2611 caracteres)\n",
      " 1 fragmentos creados.\n",
      " Fragmentos de DOCUMENTACIÓN A PRESENTAR PARA OBRA NUEVA.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: Estacionamiento-Medido.pdf\n",
      " PDF cargado correctamente (3 páginas, 2475 caracteres)\n",
      " 1 fragmentos creados.\n",
      " Fragmentos de Estacionamiento-Medido.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: Recolección de residuos.pdf\n",
      " PDF cargado correctamente (10 páginas, 9873 caracteres)\n",
      " 4 fragmentos creados.\n",
      " Fragmentos de Recolección de residuos.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: Requisitos para afectar remises.pdf\n",
      " PDF cargado correctamente (4 páginas, 4116 caracteres)\n",
      " 2 fragmentos creados.\n",
      " Fragmentos de Requisitos para afectar remises.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: Requisitos para solicitar licencia de conductor de taxis por primera vez.pdf\n",
      " PDF cargado correctamente (4 páginas, 4401 caracteres)\n",
      " 2 fragmentos creados.\n",
      " Fragmentos de Requisitos para solicitar licencia de conductor de taxis por primera vez.pdf guardados en ChromaDB.\n",
      "\n",
      " Leyendo: Requisitos_para_obtener_la_licencia__de_conducir.pdf\n",
      " PDF cargado correctamente (2 páginas, 3293 caracteres)\n",
      " 2 fragmentos creados.\n",
      " Fragmentos de Requisitos_para_obtener_la_licencia__de_conducir.pdf guardados en ChromaDB.\n",
      "\n",
      "--- INDEXACIÓN COMPLETADA: 7 archivos procesados ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- INICIANDO FASE DE INDEXACIÓN ---\")\n",
    "vector_store = get_vector_store(\"tramites_formosa\")\n",
    "archivos_indexados = 0\n",
    "for archivo in os.listdir(carpeta):\n",
    "    if archivo.endswith(\".pdf\"):\n",
    "        ruta = os.path.join(carpeta, archivo)\n",
    "        print(f\"\\n Leyendo: {archivo}\")\n",
    "\n",
    "        # 1. Cargar el PDF\n",
    "        text = upload_pdf(ruta) \n",
    "\n",
    "        if text.strip():\n",
    "            \n",
    "            # 2. Dividir el texto en fragmentos (chunks)\n",
    "            docs = split_text(text) \n",
    "            \n",
    "            if docs:\n",
    "                try:\n",
    "                    # 3. Guardar en ChromaDB\n",
    "\n",
    "                    # Esta línea genera los embeddings con Ollama y los guarda en el disco\n",
    "                    vector_store.add_documents(docs) \n",
    "                    print(f\" Fragmentos de {archivo} guardados en ChromaDB.\")\n",
    "                    archivos_indexados += 1\n",
    "                except Exception as e:\n",
    "                     # Captura fallos de Ollama o ChromaDB\n",
    "                     print(f\" Error al guardar embeddings de {archivo}: {e}\")\n",
    "            else:\n",
    "                print(f\" El archivo {archivo} se cargó, pero no generó fragmentos.\")\n",
    "        else:\n",
    "            print(f\" El archivo {archivo} no contiene texto útil.\")\n",
    "\n",
    "print(f\"\\n--- INDEXACIÓN COMPLETADA: {archivos_indexados} archivos procesados ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cec2c1",
   "metadata": {},
   "source": [
    "## 5. Función de Respuesta (Generación con Gemini)\n",
    "\n",
    "Esta celda configura el prompt y la conexión con el modelo Gemini para generar la respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68aadb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Eres un asistente encargado de responder preguntas sobre tramites de la municipalidad de Formosa.\n",
    "    Usa exclusivamente la informacion del contexto para responder al usuario.\n",
    "    contexto = {contexto}\n",
    "    pregunta del usuario: {input_user}\n",
    "                                    \n",
    "    Responde de forma clara, concisa y completa, incluyendo todos los requisitos relevantes y mencionando sedes o direcciones si aplica.\n",
    "\"\"\")\n",
    "\n",
    "def response(input_user: str, contexto: str):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        api_key=api_key,\n",
    "        model=\"gemini-2.5-flash\", \n",
    "        temperature=0.4\n",
    "    )\n",
    "    # Stream para mejor experiencia de usuario\n",
    "    print(\"\\n--- Respuesta del Asistente ---\")\n",
    "    for chunk in llm.stream(prompt.format(contexto=contexto, input_user=input_user)):\n",
    "        yield chunk.content\n",
    "    print(\"\\n-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d70dd",
   "metadata": {},
   "source": [
    "## 6. Consulta y Generación (Fase Final del RAG)\n",
    "\n",
    "Ejecuta esta celda para interactuar con tu base de conocimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user =input(\"Human: \")\n",
    "\n",
    "# 1. Recuperación (Retrieval)\n",
    "docs = retrieval(input_user=input_user)\n",
    "\n",
    "if docs and docs[0].page_content.strip(): # Verificar que se encontró contenido\n",
    "    \n",
    "    contexto = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # 2. Generación (Generation)\n",
    "    for chunk in response(input_user=input_user, contexto=contexto):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "else:\n",
    "    print(\"\\n No se encontraron documentos relevantes en la base de datos para responder a la pregunta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ddf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
